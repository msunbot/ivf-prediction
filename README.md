# üß¨ IVF Success Prediction using Machine Learning

> **Predicting IVF live birth outcomes using patient characteristics and neural networks**
A complete machine learning pipeline for predicting IVF (In Vitro Fertilization) success rates, featuring data exploration, preprocessing, baseline modeling, and three specialized neural network architectures optimized for different use cases.

---

## üìã Table of Contents
- [Overview](#overview)
- [Key Features](#key-features)
- [Dataset](#dataset)
- [Model Zoo](#model-zoo)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Results](#results)
- [Lessons Learned](#lessons-learned)
- [Future Work](#future-work)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgments](#acknowledgments)

---

## üéØ Overview

This project tackles the challenging problem of predicting IVF live birth success using machine learning. IVF outcomes depend on numerous interacting factors including:

- **Patient demographics**: Age, BMI, years of infertility
- **Hormonal markers**: AMH, FSH, LH, Estrogen, Progesterone
- **Uterine factors**: Endometrial thickness, AFC count
- **Treatment details**: Protocol type, embryo quality, transfer type
- **Lifestyle factors**: Diet, exercise, stress levels, sleep
- **Male factors**: Sperm parameters (count, motility, morphology)

The project demonstrates a complete ML workflow from data exploration to model deployment, with emphasis on understanding trade-offs between different modeling approaches.

---

## ‚ú® Key Features

- **üîç Comprehensive Data Exploration**: Detailed analysis of 2,000 patient records with 40+ features
- **üßπ Domain-Aware Preprocessing**: Intelligent handling of missing values based on medical context
- **üìä Multiple Model Architectures**: Three specialized models optimized for different priorities
- **‚öñÔ∏è Trade-off Analysis**: Demonstrates when to prioritize AUC vs. recall vs. generalization
- **üìà Extensive Visualization**: Training curves, confusion matrices, ROC curves, prediction distributions
- **üè• Clinical Focus**: Considers real-world implications of false positives vs. false negatives
- **üìù Well-Documented**: Extensive comments, learning log, and analysis of experiments

---

## üìä Dataset

**Source**: Kaggle IVF Success Prediction Dataset  
**Size**: 2,000 patients  
**Features**: 40+ clinical, lifestyle, and treatment variables  
**Target**: Live birth success (binary: 0/1)  
**Class Distribution**: 51.4% success, 48.6% failure (balanced)

### Key Features Used:
```
Numerical (24):
‚îú‚îÄ‚îÄ Patient Demographics: Age, BMI, Years_of_Infertility
‚îú‚îÄ‚îÄ Hormonal Markers: AMH_Level, FSH_Level, LH_Level, Estrogen_E2, Progesterone_P4
‚îú‚îÄ‚îÄ Uterine Factors: Endometrial_Thickness_mm, AFC_Count
‚îú‚îÄ‚îÄ Metabolic: Thyroid_TSH, Insulin_Level
‚îú‚îÄ‚îÄ Treatment: Number_of_IVF_Cycles, Pregnancy_History, Number_of_Embryos_Transferred
‚îú‚îÄ‚îÄ Lifestyle: Diet_Quality_Score, Yoga_Sessions, Stress_Level, Physical_Activity, Sleep_Duration
‚îî‚îÄ‚îÄ Male Factors: Sperm_Count, Sperm_Motility, Sperm_Morphology, Sperm_DNA_Fragmentation

Categorical (13):
‚îú‚îÄ‚îÄ Medical: Diagnosis_Type, Male_Infertility_Diagnosis
‚îú‚îÄ‚îÄ Treatment: Medication_Protocol, Transfer_Type, ICSI_or_IVF, Day_of_Transfer
‚îú‚îÄ‚îÄ Procedures: Luteal_Support_Given, PGT, Assisted_Hatching_Used
‚îî‚îÄ‚îÄ Lifestyle: Smoking_Status, Alcohol_Consumption, Exposure_to_Environmental_Toxins, Occupation_Type
```

---

## üèÜ Model Zoo

This project includes **three specialized models**, each optimized for different priorities:

| Model | Architecture | Use Case | AUC | Recall | Parameters | Speed |
|-------|-------------|----------|-----|--------|------------|-------|
| **Balanced** | 37‚Üí32‚Üí16‚Üí1 | Academic benchmark | **0.5425** | 0.519 | 1,761 | Medium |
| **Clinical** | 37‚Üí32‚Üí16‚Üí1 | Max viable cases | 0.5323 | **0.636** | 1,761 | Medium |
| **Production** | 37‚Üí24‚Üí1 | Edge deployment | 0.5287 | 0.481 | **937** | **Fast** |

### 1Ô∏è‚É£ Balanced Model (Recommended for Research)
```python
Use when: Maximizing AUC, academic benchmarking
‚úì Highest AUC (0.5425, +4.2% vs baseline)
‚úì Best overall performance (55% accuracy)
‚úì Balanced precision/recall
‚ö† More parameters (1,761)
```

### 2Ô∏è‚É£ Clinical Model (Recommended for Healthcare)
```python
Use when: Clinical decision support, don't miss viable cases
‚úì Highest recall (0.636 - catches 64% of successes!)
‚úì 22% better at finding viable IVF candidates than balanced model
‚úì Lower false negative rate (important for patient decisions)
‚ö† Slightly lower AUC (0.5323)
```

### 3Ô∏è‚É£ Production Model (Recommended for Deployment)
```python
Use when: Mobile apps, edge devices, resource constraints
‚úì Fastest inference (47% fewer parameters)
‚úì Best generalization (overfitting gap: 0.0256)
‚úì Most stable training
‚ö† Lower accuracy (50%)
```

---

## üîß Installation

### Prerequisites
- Python 3.8+
- pip package manager

### Setup

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/ivf-prediction.git
cd ivf-prediction
```

2. **Create virtual environment** (recommended)
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

### Requirements
```txt
numpy>=1.21.0
pandas>=1.3.0
matplotlib>=3.4.0
seaborn>=0.11.0
scikit-learn>=1.0.0
torch>=2.0.0
joblib>=1.0.0
```

---

## üöÄ Quick Start

### Option 1: Run Complete Pipeline
```bash
# Step 1: Data exploration
python 01_data_exploration.py

# Step 2: Baseline model (Logistic Regression)
python 02_preprocessing_baseline.py

# Step 3: Neural network (choose model type inside script)
python 03_neural_network.py
```

### Option 2: Train Specific Model
```python
from models.model_configs import get_model, get_config

# Choose model type: 'balanced', 'clinical', or 'production'
model = get_model('clinical', input_size=37)
config = get_config('clinical')

# Train with config parameters
# See 03_neural_network.py for full training code
```

### Option 3: Make Predictions
```python
import torch
from models.model_configs import get_model

# Load trained model
model = get_model('balanced', input_size=37)
model.load_state_dict(torch.load('models/best_model_balanced.pth'))
model.eval()

# Prepare your data (must match training preprocessing)
# X_new = preprocess(your_data)

# Make predictions
with torch.no_grad():
    probabilities = model(X_new)
    predictions = (probabilities >= 0.5).float()
```

---

## üìÅ Project Structure

```
ivf-prediction/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ kaggle_data_set.csv              # Raw dataset
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ model_configs.py                 # Model definitions & configs
‚îÇ   ‚îú‚îÄ‚îÄ best_model_balanced.pth          # Trained: Balanced model
‚îÇ   ‚îú‚îÄ‚îÄ best_model_clinical.pth          # Trained: Clinical model
‚îÇ   ‚îú‚îÄ‚îÄ best_model_production.pth        # Trained: Production model
‚îÇ   ‚îú‚îÄ‚îÄ logistic_regression_model.pkl    # Baseline model
‚îÇ   ‚îú‚îÄ‚îÄ scaler.pkl                       # Feature scaler
‚îÇ   ‚îî‚îÄ‚îÄ label_encoders.pkl               # Categorical encoders
‚îÇ
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ ivf_kaggle_exploration.png       # EDA visualizations
‚îÇ   ‚îú‚îÄ‚îÄ baseline_model_evaluation_kaggle.png
‚îÇ   ‚îî‚îÄ‚îÄ neural_network_results_kaggle.png
‚îÇ
‚îú‚îÄ‚îÄ 01_exploration.py                    # Data exploration
‚îú‚îÄ‚îÄ 02_preprocessing_baseline.py         # Baseline (Logistic Regression)
‚îú‚îÄ‚îÄ 03_neural_network.py                 # Neural network training
‚îú‚îÄ‚îÄ requirements.txt                     # Python dependencies
‚îú‚îÄ‚îÄ README.md                            # This file
‚îú‚îÄ‚îÄ LEARNING_LOG.md                      # Detailed learning journey
‚îî‚îÄ‚îÄ .gitignore                           # Git ignore rules
```

---

## üìà Results

### Model Performance Comparison

| Metric | Baseline (LR) | Balanced NN | Clinical NN | Production NN |
|--------|--------------|-------------|-------------|---------------|
| **AUC** | 0.5203 | **0.5425** (+4.2%) | 0.5323 (+2.3%) | 0.5287 (+1.6%) |
| **Accuracy** | 52.3% | **55.0%** | 54.75% | 50.0% |
| **Precision** | 0.534 | **0.569** | 0.553 | 0.516 |
| **Recall** | 0.512 | 0.519 | **0.636** | 0.481 |
| **F1-Score** | 0.523 | 0.543 | **0.591** | 0.498 |
| **Overfitting Gap** | N/A | 0.0766 | 0.0422 | **0.0256** |
| **Parameters** | N/A | 1,761 | 1,761 | **937** |

### Key Findings

1. **All neural networks beat the baseline** by 1.6-4.2% in AUC
2. **Clinical model catches 22% more successes** than balanced model (recall: 0.636 vs 0.519)
3. **Production model has best generalization** (lowest overfitting)
4. **Top 5 predictive features** (by correlation with outcome):
   - Estrogen_E2_Level (+)
   - Age (-)
   - AMH_Level (+)
   - Number_of_Embryos_Transferred (+)
   - Endometrial_Thickness_mm (+)

### Trade-off Analysis

**Choosing Clinical over Balanced:**
- ‚úÖ +11.7% improvement in recall (0.519 ‚Üí 0.636)
- ‚úÖ +8.8% improvement in F1-score
- ‚úÖ 45% reduction in overfitting
- ‚ö†Ô∏è -1.8% decrease in AUC

**For IVF prediction, this trade-off is worthwhile** because:
- Missing a viable case (false negative) = patient may give up
- False positive = patient tries again (no harm, just emotional impact)

---

## üí° Lessons Learned

### Technical Insights

1. **Simpler is Often Better**
   - Started with 128‚Üí64‚Üí32 architecture (15K parameters)
   - Failed spectacularly (AUC 0.504, severe overfitting)
   - Simplified to 32‚Üí16 (1.7K parameters)
   - **Result**: +8.2% AUC improvement, 45% less overfitting

2. **Class Imbalance Handling**
   - Dataset is nearly balanced (51/49 split)
   - Adding `pos_weight` to loss function made predictions worse
   - **Lesson**: Don't blindly apply techniques; check if problem exists first

3. **Missing Data Context Matters**
   - 689 missing values for "Male_Infertility_Diagnosis"
   - Naive approach: Fill with most common value ("Varicocele")
   - **Problem**: Labels 689 healthy men as having a condition!
   - **Solution**: Fill with "None" (missing = no diagnosis)
   - **Impact**: Model learned truthful patterns

4. **Multiple Metrics Matter**
   - AUC isn't everything
   - For clinical applications, recall can be more important
   - Always consider the **cost of different error types**

5. **Overfitting Detection**
   - Train/val gap > 0.10 = severe overfitting
   - Solutions: Reduce model size, increase dropout, add regularization
   - Early stopping is essential

### ML Best Practices Demonstrated

‚úÖ Always split data BEFORE scaling (prevent data leakage)  
‚úÖ Use stratified splitting for balanced train/test sets  
‚úÖ Implement early stopping to prevent overfitting  
‚úÖ Start with simple baseline (logistic regression)  
‚úÖ Monitor both training AND validation metrics  
‚úÖ Consider domain knowledge when preprocessing  
‚úÖ Document experiments (successful AND failed)  
‚úÖ Create multiple models for different use cases  

---

## üöß Future Work

### Short-term Improvements
- [ ] **Cross-validation**: 5-fold CV for more robust evaluation
- [ ] **Hyperparameter optimization**: Grid search or Optuna
- [ ] **Feature engineering**: Create interaction terms (Age √ó AMH, BMI √ó FSH)
- [ ] **Ensemble methods**: Combine multiple models (stacking)
- [ ] **Explainability**: SHAP values for feature importance

### Long-term Extensions
- [ ] **Multi-task learning**: Predict clinical pregnancy, miscarriage, twins
- [ ] **Survival analysis**: Time-to-pregnancy modeling
- [ ] **Web interface**: Streamlit or Flask app for predictions
- [ ] **Transfer learning**: Fine-tune on clinic-specific data
- [ ] **Mobile deployment**: TensorFlow Lite for on-device inference
- [ ] **Fairness analysis**: Check for demographic biases

---

## ü§ù Contributing

Contributions are welcome! This project was built as a learning exercise, and suggestions for improvement are appreciated.

### How to Contribute
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/improvement`)
3. Commit your changes (`git commit -am 'Add improvement'`)
4. Push to the branch (`git push origin feature/improvement`)
5. Open a Pull Request

### Areas for Contribution
- Additional model architectures
- Better visualization techniques
- Deployment examples (Docker, cloud services)
- Unit tests for preprocessing functions
- Documentation improvements

---

## üìù License

This project is licensed under the MIT License

---

## üôè Acknowledgments

- **Kaggle** for providing the IVF prediction dataset
- **Deep Learning Specialization (Coursera)** for foundational ML knowledge
- **PyTorch** and **scikit-learn** communities for excellent documentation
- **IVF research community** for domain expertise

---

## üìß Contact

**Author**: Michelle Sun  
**GitHub**: [@msunbot](https://github.com/msunbot)  

---

## üåü Star This Project

If you found this project helpful or interesting, please consider giving it a star! ‚≠ê

It helps others discover the project and motivates continued development.

---

**Built with ‚ù§Ô∏è as a learning project in machine learning and healthcare AI**
