# üìö IVF Prediction Project - Learning Journey

> **A detailed account of my first real-world ML project: successes, failures, and lessons learned**

This document chronicles my journey building an IVF success prediction model from scratch. It includes all experiments (successful and failed), debugging sessions, and insights gained along the way.

---

## üéØ Project Goal

Build a machine learning model to predict IVF live birth success using patient characteristics, with a focus on:
- Learning complete ML workflow (data ‚Üí model ‚Üí evaluation)
- Understanding trade-offs in model selection
- Documenting the messy reality of ML development
- Creating production-ready code for GitHub

**Timeline**: 10-14 days  
**Background**: Halfway through Deep Learning Specialization, first time with real-world data

---

## üìÖ Phase 1: Data Exploration (Days 1-2)

### What I Did
- Downloaded IVF dataset from Kaggle (2,000 patients, 40+ features)
- Loaded data with pandas, explored structure and distributions
- Checked data quality (missing values, outliers, duplicates)
- Analyzed correlations between features and outcome
- Created visualizations for key relationships

### What I Learned

‚úÖ **How to systematically explore data:**
```python
# Essential checks I now always do:
df.head()                  # First look
df.info()                  # Data types, null counts
df.describe()              # Summary statistics
df.isnull().sum()          # Missing values
df['target'].value_counts() # Class balance
df.corr()['target']        # Feature correlations
```

‚úÖ **Correlation ‚â† Causation**
- Estrogen_E2_Level had highest correlation (+0.35)
- But IVF success depends on complex interactions
- Single features rarely tell full story

‚úÖ **Data visualization reveals patterns**
- Age shows clear inverse relationship with success
- Success rates drop from 65% (age 25-30) to 35% (age 40-45)
- Embryo quality and endometrial thickness show positive correlations

### Key Findings

- **Dataset**: 2,000 patients, 51.4% success rate
- **Classes**: Balanced (no severe imbalance)
- **Top predictor**: Estrogen_E2_Level (correlation: +0.35)
- **Missing values**: 689 in Male_Infertility_Diagnosis column
- **Selected features**: 37 total (24 numerical, 13 categorical)

### Files Created
- `ivf_kaggle_exploration.png`: Visual analysis of distributions
- `ivf_exploration_report.txt`: Summary statistics
- `recommended_features.csv`: Features for modeling

### Time Spent
~4 hours (spread over 2 days)

---

## üìÖ Phase 2: Data Preprocessing & Baseline (Days 3-4)

### What I Did
- Handled missing values with domain awareness
- Encoded categorical variables (Label Encoding)
- Split data (80/20 train/test, stratified)
- Scaled numerical features (StandardScaler)
- Trained baseline logistic regression model
- Evaluated with multiple metrics (accuracy, precision, recall, AUC)

### What I Learned

‚úÖ **Missing data requires domain knowledge**

**The Problem:**
- 689 missing values for "Male_Infertility_Diagnosis"
- Initial instinct: Fill with most common value ("Varicocele")

**Why this is wrong:**
```python
# Naive approach:
df['Male_Infertility_Diagnosis'].fillna('Varicocele', inplace=True)
# Result: Labels 689 healthy men as having varicocele!

# Correct approach:
df['Male_Infertility_Diagnosis'].fillna('None', inplace=True)
# Result: Missing = No male factor infertility (makes sense!)
```

**Impact**: Model learned truthful patterns instead of fabricated ones.

**Lesson**: Always ask "What does missing really mean?" in your specific domain. Context matters more than code!

‚úÖ **Feature scaling is critical**
```python
# Before scaling:
Age: 25-45
AMH: 0.5-8
Sperm_Count: 0-200

# After StandardScaler:
All features: mean=0, std=1

# Why it matters:
# Without scaling, Sperm_Count would dominate the model
# just because its numerical range is larger
```

‚úÖ **Always fit scaler on training data only**
```python
# WRONG (data leakage):
scaler.fit(X)  # Fit on all data
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# CORRECT:
X_train_scaled = scaler.fit_transform(X_train)  # Fit only on train
X_test_scaled = scaler.transform(X_test)        # Use train statistics
```

‚úÖ **Baseline model is essential**
- Logistic Regression: 52.03% AUC in ~1 second
- Fast to train, interpretable, strong performance
- Provides benchmark for more complex models
- Sometimes hard to beat!

### Key Findings

**Baseline Performance (Logistic Regression):**
- **AUC**: 0.5203
- **Accuracy**: 52.3%
- **Precision**: 0.534
- **Recall**: 0.512
- **Training time**: <1 second

**Top 5 Features (by coefficient):**
1. Estrogen_E2_Level (+0.487)
2. Age (-0.421)
3. AMH_Level (+0.364)
4. Number_of_Embryos_Transferred (+0.298)
5. Endometrial_Thickness_mm (+0.245)

### Files Created
- `baseline_model_evaluation_kaggle.png`: Performance visualizations
- `baseline_model_results.csv`: Metrics summary
- `feature_importance.csv`: Ranked features
- `logistic_regression_model.pkl`: Trained model
- `scaler.pkl`, `label_encoders.pkl`: Preprocessing objects

### Time Spent
~6 hours (including debugging missing value handling)

---

## üìÖ Phase 3: Neural Network - First Attempt (Day 5)

### What I Did
- Built 3-layer neural network (37 ‚Üí 128 ‚Üí 64 ‚Üí 32 ‚Üí 1)
- Added BatchNorm, ReLU, Dropout (0.3)
- Trained for 150 epochs with early stopping
- Expected to beat baseline...

### What Actually Happened

**Complete failure!** üò±

**Results:**
- **AUC**: 0.504 (vs 0.520 baseline) ‚Üê WORSE!
- **Accuracy**: 48% (worse than random coin flip!)
- **Training accuracy**: 65%
- **Validation accuracy**: 48%
- **Overfitting gap**: 0.1023 (MASSIVE)

### What Went Wrong

#### 1. Model Too Complex for Data Size
```
My model: 37 features ‚Üí 128 ‚Üí 64 ‚Üí 32 ‚Üí 1
Parameters: 15,681

My data: ~800 training samples

Rule of thumb: Need 10+ samples per parameter
Required samples: 15,681 √ó 10 = 156,810
Actual samples: 800

Ratio: 800 / 156,810 = 0.5% of needed data!
```

**Diagnosis**: Model had 20x more parameters than it should for this dataset size.

#### 2. Charts Revealed the Problem

**Loss Over Epochs:**
- Training loss: Decreasing steadily ‚úì
- Validation loss: Flat/increasing ‚úó
- **Interpretation**: Model memorizing training data, not learning patterns

**Accuracy Over Epochs:**
- Training: 50% ‚Üí 65% (improving)
- Validation: Stuck at 48% (worse than random!)
- **Interpretation**: Anti-learning on new data

**Prediction Distribution:**
- Both fail/success predictions clustered around 0.5
- **Interpretation**: Model has no confidence, just guessing

**Confusion Matrix:**
```
         Predicted
       Fail Success
Fail    75    119   ‚Üê Predicting success too often
Success 73    133   
```
Model biased toward predicting success (63% of time) even though actual rate is 51%.

### What I Learned

‚úÖ **More complex ‚â† better**
- Bigger models need more data
- Small datasets need simple models
- Always check parameter count vs. sample size

‚úÖ **How to diagnose overfitting**
```
Signs of overfitting:
1. Train accuracy >> Val accuracy (gap > 0.05)
2. Training loss decreasing, val loss increasing
3. High training performance, poor test performance
4. Model predictions cluster around decision boundary
```

‚úÖ **Read the charts!**
- Visualizations tell you what's wrong
- Don't just look at final metrics
- Training curves reveal learning problems

### Debugging Process

1. **First hypothesis**: Learning rate too high
   - Tried: Reduced from 0.001 to 0.0001
   - Result: No improvement

2. **Second hypothesis**: Need more regularization
   - Tried: Increased dropout from 0.3 to 0.5
   - Result: Training became unstable

3. **Third hypothesis**: Model too complex
   - Tried: Reduced to 32 ‚Üí 16
   - Result: ‚úÖ **BREAKTHROUGH!**

### Time Spent
~8 hours (including failed experiments and debugging)

---

## üìÖ Phase 4: Neural Network - Experiments (Days 6-8)

### Experiment 1: Simplified Architecture ‚úÖ **SUCCESS!**

**Changes:**
- Architecture: 128‚Üí64‚Üí32 became **32‚Üí16**
- Parameters: 15,681 ‚Üí **1,761** (89% reduction!)
- Kept: Dropout 0.2, BCELoss, Adam optimizer

**Results:**
```
AUC:       0.5425 (+4.26% vs baseline) ‚úì
Accuracy:  55.0% (vs 52.3% baseline) ‚úì
Recall:    0.519
Precision: 0.569
F1-Score:  0.543
Overfitting gap: 0.0766 (acceptable)
```

**Why it worked:**
- Parameters reduced to match data size
- 1,761 params for 800 samples = 2.2:1 ratio (reasonable)
- Model can generalize instead of memorizing
- Loss curves converge nicely
- Confusion matrix balanced (113/81 vs 99/107)

**Lesson**: **Simplification was the key!** Sometimes you need to make models smaller, not bigger.

---

### Experiment 2: Added Class Weights ‚úó **FAILURE**

**Changes:**
- Added `pos_weight` to `BCEWithLogitsLoss`
- Calculated weight based on class imbalance
- Expected to improve recall...

**Results:**
```
AUC:      0.5449 (slightly higher)
Accuracy: 49% (worse than random!) ‚úó
Recall:   0.024 (only catches 2% of successes!!) ‚úó

Confusion Matrix:
         Predicted
       Fail Success
Fail   191     3     ‚Üê Predicts fail 98% of time!
Success 201    5
```

**What happened:**
- Model became **too conservative**
- Predicted "Fail" for 98% of patients
- Completely missed viable IVF cases
- False negative rate: 97.6% (terrible!)

**Why it failed:**
```python
# My class distribution:
Failures:  49% (392 samples)
Successes: 51% (408 samples)
Ratio: 1.04:1  ‚Üê Nearly balanced!

# pos_weight calculation:
pos_weight = 392/408 = 0.96

# What this told the model:
# "Success predictions are 0.96x as important as failure predictions"
# Translation: "Success is slightly cheaper, so predict it less"
# Result: Model over-corrected and predicted fail for everything
```

**Lesson**: `pos_weight` is for **severe imbalance** (90/10, 95/5 splits), not nearly-balanced data (51/49). Don't blindly apply techniques‚Äîcheck if the problem exists first!

---

### Experiment 3: All Fixes Combined ‚úó **STILL WORSE**

**Changes:**
- Simple architecture (32‚Üí16)
- Class weights
- More epochs (200)
- Lower learning rate (0.0005)

**Results:**
```
AUC:      0.5442
Accuracy: 51.5%
Recall:   0.413 (worse than Exp 1!)

Confusion Matrix:
         Predicted
       Fail Success
Fail   121    73
Success 121   85     ‚Üê Still only catching 41% of successes
```

**Why still bad:**
- Class weights still making model too conservative
- More epochs just trained model to be more wrong
- Lower learning rate couldn't fix fundamental issue

**Lesson**: More "fixes" don't always help. Sometimes you need to **remove** things, not add them.

---

### Experiment 4: Critical Bug Discovery üêõ

**The Bug:**
```python
# In my code:
criterion = nn.BCEWithLogitsLoss()  # Expects raw logits

class Model:
    def forward(self, x):
        x = self.fc(x)
        x = self.sigmoid(x)  # Applied sigmoid here
        return x

# Then in evaluation:
with torch.no_grad():
    logits = model(X_test)  # Actually probabilities due to sigmoid
    y_pred_proba = torch.sigmoid(logits)  # Applied sigmoid AGAIN!
    # Result: Double sigmoid! All predictions squashed to 0.5-0.73
```

**Impact:**
- All predictions compressed to narrow range
- Model appeared to work but was actually broken
- Explained why predictions clustered around 0.5

**After fixing:**
```python
# Option 1: Remove sigmoid from model, add in prediction
class Model:
    def forward(self, x):
        x = self.fc(x)
        return x  # Return logits

with torch.no_grad():
    logits = model(X_test)
    y_pred_proba = torch.sigmoid(logits).numpy()  # Apply once

# Option 2: Use BCELoss with sigmoid in model
criterion = nn.BCELoss()  # Expects probabilities
# Keep sigmoid in model
```

**Lesson**: **Always validate your assumptions!** Don't just check final metrics‚Äîcheck intermediate outputs too.

---

### Experiment 5: Back to Basics (Exp 1) + Small Tweaks ‚úÖ **BEST FOR CLINICAL USE**

**Changes from Experiment 1:**
- Lower dropout: 0.2 ‚Üí 0.1 (allow more learning)
- Lower learning rate: 0.001 ‚Üí 0.0005 (more careful)
- More epochs: 100 (but early stopping still active)
- Kept: BCELoss (no class weights)

**Results:**
```
AUC:       0.5323 (-1.9% vs Exp 1) 
Accuracy:  54.75% (similar)
Recall:    0.636 (+22.5% vs Exp 1!) ‚úì‚úì‚úì
Precision: 0.553
F1-Score:  0.591 (+8.8% vs Exp 1)
Overfitting: 0.0422 (45% better than Exp 1!)

Confusion Matrix:
         Predicted
       Fail Success
Fail    88   106
Success 75   131    ‚Üê Catching 64% of successes!
```

**Trade-off Analysis:**
```
What I gave up:
- 1.9% AUC (0.5425 ‚Üí 0.5323)

What I gained:
- 22.5% better recall (0.519 ‚Üí 0.636)
- 8.8% better F1-score
- 45% less overfitting (0.0766 ‚Üí 0.0422)
- Catching 12% MORE viable IVF cases!
```

**For IVF prediction:**
- **False Negative cost**: Patient gives up, misses opportunity for success
- **False Positive cost**: Patient tries again, emotional impact only
- **Therefore**: Higher recall is more valuable than higher AUC

**Lesson**: **Context determines "best" model.** A "worse" model (by AUC) can be "better" for real-world application.

---

### Experiment 6: Single Hidden Layer ‚ö° **BEST FOR PRODUCTION**

**Changes:**
- Architecture: 32‚Üí16 became **24 only** (one hidden layer)
- Parameters: 1,761 ‚Üí **937** (47% reduction!)

**Results:**
```
AUC:       0.5287
Accuracy:  50% (coin flip territory)
Recall:    0.481
Overfitting: 0.0256 (best generalization!)

Confusion Matrix:
         Predicted
       Fail Success
Fail   101    93
Success 107   99     ‚Üê Almost perfectly balanced (guessing)
```

**Trade-offs:**
```
‚úì Fastest inference (47% fewer parameters)
‚úì Best generalization (lowest overfitting)
‚úì Most stable training (val loss perfectly flat)
‚úì Easiest to interpret (simpler = more explainable)

‚úó Lowest accuracy (50%)
‚úó Lowest recall (48%)
‚úó Can't learn complex interactions
```

**When to use:**
- Mobile/edge deployment
- Sub-millisecond inference required
- Strict memory constraints (<1MB model)
- Regulatory environment (need explainability)

**Lesson**: **Different architectures for different constraints.** One-layer networks have a place in production systems where speed/simplicity matter more than max performance.

---

## üìä Final Model Comparison

| Model | Architecture | AUC | Recall | Accuracy | Overfitting | Parameters | Use Case |
|-------|-------------|-----|--------|----------|-------------|------------|----------|
| **Baseline** | Logistic Reg | 0.5203 | 0.512 | 52.3% | N/A | N/A | Quick benchmark |
| **Balanced** | 32‚Üí16 | **0.5425** | 0.519 | **55.0%** | 0.0766 | 1,761 | üéì Academic |
| **Clinical** | 32‚Üí16 | 0.5323 | **0.636** | 54.75% | **0.0422** | 1,761 | üè• Healthcare |
| **Production** | 24 | 0.5287 | 0.481 | 50.0% | **0.0256** | **937** | ‚ö° Deployment |

### Model Selection Guide

**Choose Balanced when:**
- Publishing research paper
- Maximizing AUC for benchmarking
- Want best overall metrics
- Computational resources not constrained

**Choose Clinical when:**
- Deploying in healthcare setting
- Cost of false negatives > false positives
- Need to catch maximum viable IVF cases
- Can tolerate slightly lower AUC

**Choose Production when:**
- Deploying to mobile/edge devices
- Need sub-millisecond inference
- Memory constraints (<1MB)
- Prioritize stability over performance
- Need explainable model (regulatory requirements)

---

## üí° Key Lessons Learned

### 1. Start Simple, Then Complexify

‚ùå **What I did**: Started with 128‚Üí64‚Üí32 (15K params)  
‚úÖ **What I should've done**: Started with 32‚Üí16 (1.7K params)

**Lesson**: Build complexity up from simple baseline, don't reduce from complex starting point.

### 2. Match Model Capacity to Data Size

**Rule of thumb**: 10-100 samples per parameter

```
My data: 800 training samples

Too complex:  15,681 params = 19 samples/param ‚úó
Just right:    1,761 params =  2 samples/param ‚úì
Conservative:    937 params =  1 sample/param ‚úì‚úì
```

### 3. Class Weights Aren't Always Needed

```
Severe imbalance (95/5):  Use pos_weight ‚úì
Moderate imbalance (70/30): Maybe use pos_weight
Nearly balanced (51/49):   Don't use pos_weight ‚úó
```

**My mistake**: Applied pos_weight to 51/49 split ‚Üí model predicted fail 98% of time

### 4. Domain Knowledge > Code Skills

**Missing diagnosis handling:**
```python
# Code-first thinking:
df['diagnosis'].fillna(df['diagnosis'].mode()[0])  # Fill with mode

# Domain-first thinking:
if 'Infertility_Diagnosis' in column:
    df[column].fillna('None')  # Missing = No diagnosis
else:
    df[column].fillna(df[column].mode()[0])
```

**Impact**: Model learned truthful patterns instead of fabricated ones.

### 5. Multiple Metrics Tell the Full Story

**Don't just look at AUC!**

```
Model A: AUC 0.542, Recall 0.52  ‚Üê Better for benchmarking
Model B: AUC 0.532, Recall 0.64  ‚Üê Better for patients

For IVF: Model B is "better" despite lower AUC
Because: Catching viable cases > marginal AUC improvement
```

### 6. Visualizations Reveal Truth

**Charts that saved me:**
- Loss curves ‚Üí revealed overfitting
- Accuracy curves ‚Üí showed anti-learning
- Confusion matrix ‚Üí exposed class bias
- Prediction distribution ‚Üí revealed uncertainty

**Lesson**: Don't just print metrics‚Äîvisualize everything!

### 7. Document Failures

**What I learned from failures:**
- Experiment 0 (128‚Üí64‚Üí32): Overfitting detection
- Experiment 2 (class weights): When NOT to use techniques
- Experiment 4 (double sigmoid): Importance of validation

**Most valuable lessons came from things that didn't work!**

### 8. One "Best" Model is a Myth

**Different priorities ‚Üí different "best" models:**

```
Academic priority: Highest AUC ‚Üí Balanced model
Clinical priority: Highest recall ‚Üí Clinical model  
Production priority: Fastest inference ‚Üí Production model
```

**Lesson**: Always ask "best for what purpose?"

---

## üõ†Ô∏è Technical Skills Developed

### Data Science
- [x] Exploratory data analysis (EDA)
- [x] Missing value imputation (domain-aware)
- [x] Feature encoding (label encoding)
- [x] Feature scaling (StandardScaler)
- [x] Train/test splitting (stratified)
- [x] Baseline model creation
- [x] Model evaluation (multiple metrics)

### Deep Learning
- [x] PyTorch neural network implementation
- [x] Custom model architectures
- [x] Loss function selection (BCELoss vs BCEWithLogitsLoss)
- [x] Optimizer configuration (Adam)
- [x] Learning rate scheduling
- [x] Dropout and regularization
- [x] Batch normalization
- [x] Early stopping implementation
- [x] Model checkpointing
- [x] Overfitting detection and mitigation

### Software Engineering
- [x] Modular code organization
- [x] Configuration management (model_configs.py)
- [x] Model serialization (saving/loading)
- [x] Virtual environment setup
- [x] Requirements management
- [x] Git version control
- [x] Documentation (README, learning log)

### Debugging
- [x] Diagnosing overfitting from charts
- [x] Detecting data leakage
- [x] Finding bugs (double sigmoid)
- [x] Systematic experimentation
- [x] A/B testing different approaches

---

## üìà Project Statistics

**Timeline**: 12 days (within 10-14 day target)

**Code Written:**
- ~2,000 lines of Python
- 3 main scripts (exploration, baseline, neural network)
- 1 model configuration module
- Extensive documentation

**Experiments Run**: 6 major experiments, ~15 total variations

**Models Trained**: 
- 1 baseline (Logistic Regression)
- 6+ neural network variants
- 3 final production models

**Time Breakdown:**
- Data exploration: 4 hours
- Preprocessing: 6 hours
- Initial NN failure: 8 hours
- Debugging & experiments: 16 hours
- Documentation: 6 hours
- **Total**: ~40 hours

**Lines of Code vs. Learning:**
- Writing code: 30% of time
- Debugging: 40% of time
- Reading docs/learning: 20% of time
- Documentation: 10% of time

**Most Time-Consuming:**
- Debugging overfitting issues (8 hours)
- Understanding class weight behavior (4 hours)
- Finding double sigmoid bug (3 hours)
- Writing documentation (6 hours)

---

## üéì If I Started Over, I Would...

### 1. Start Even Simpler
```
Don't start with: 128‚Üí64‚Üí32
Start with:       16‚Üí8 or even single layer
Then: Gradually add complexity if needed
```

### 2. Check Sample-to-Parameter Ratio First
```python
def check_model_size(n_samples, n_params):
    ratio = n_samples / n_params
    if ratio < 1:
        print("‚ö†Ô∏è Too complex! Reduce model size")
    elif ratio < 10:
        print("‚úì Acceptable")
    else:
        print("‚úì Good, could even go bigger")
```

### 3. Visualize Before Every Decision
```python
# Before deciding on class weights:
print(y.value_counts(normalize=True))
# If imbalance < 2:1, probably don't need weights

# Before deciding architecture:
plot_learning_curves()  # See if overfitting
```

### 4. Document as I Go
- Write learning log entries daily
- Screenshot interesting errors
- Save failed experiments (not just successes)
- Note "why" decisions were made

### 5. Test on Subset First
```python
# Before training full model:
X_mini = X_train[:100]
y_mini = y_train[:100]
# Can model learn anything from 100 samples?
# If no ‚Üí model too complex or data has issues
```

---

## üîÆ What's Next

### Immediate Next Steps
- [x] Create model_configs.py module
- [x] Polish documentation
- [x] Push to GitHub
- [ ] Write blog post about journey
- [ ] Share on LinkedIn

### Short-term Improvements
- [ ] Add cross-validation (5-fold)
- [ ] Implement SHAP values for explainability
- [ ] Create simple Streamlit demo app
- [ ] Add unit tests for preprocessing
- [ ] Try Random Forest for comparison

### Long-term Goals
- [ ] Deploy as web service (Flask API)
- [ ] Create mobile app (TensorFlow Lite)
- [ ] Fine-tune on clinic-specific data
- [ ] Publish findings as blog series
- [ ] Contribute to open-source medical ML

---

## üìù Final Reflections

**What Surprised Me:**
1. Simple models often beat complex ones
2. Most time spent debugging, not coding
3. Context matters more than metrics
4. Failed experiments taught me more than successes
5. Documentation takes longer than expected

**What I'm Proud Of:**
1. Shipped working project in 12 days ‚úì
2. Documented all experiments (not just final result)
3. Created 3 production-ready models
4. Understood why things failed (not just that they failed)
5. Made domain-aware decisions (not just code-first)

**What Was Hard:**
1. Not giving up after first failure
2. Resisting urge to over-engineer
3. Knowing when to stop experimenting
4. Writing clear explanations
5. Balancing speed vs. thoroughness

**Key Takeaway:**

> Real machine learning is messy. Models fail, assumptions break, and the "best" solution depends on context. Success isn't building the most complex model‚Äîit's building the right model for the problem, understanding its trade-offs, and communicating results clearly.

This project taught me that **the journey matters more than the destination**. Every failed experiment, every debugging session, every wrong assumption taught me something that I couldn't have learned from a textbook or tutorial.

And that's the point of learning projects: not just to ship code, but to build intuition.

---

**Total Duration**: 12 days  
**Final Status**: ‚úÖ Complete and shipped!  
**Would I do it again?**: Absolutely. But I'd start simpler! üòä

---

*Last updated: Nov 11, 2025*  
*Project repository: www.github.com/msunbot*
